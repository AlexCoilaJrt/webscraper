# Configuraci√≥n de LLM Gratuito para el Chatbot

El chatbot ahora soporta m√∫ltiples APIs gratuitas de LLM. **Para usar el LLM necesitas obtener una API key gratuita** de uno de estos proveedores.

## üöÄ Opci√≥n 1: Groq (Recomendada - Gratuita y Muy R√°pida)

Groq ofrece APIs gratuitas con modelos como Llama 3.1, Mixtral, y Gemma. Es la opci√≥n m√°s r√°pida.

### Pasos:

1. **Obtener API Key (Gratuita):**
   - Ve a https://console.groq.com/
   - Crea una cuenta gratuita (solo necesitas email)
   - Ve a "API Keys" en el men√∫
   - Genera una nueva API key
   - Copia la key (empieza con `gsk_...`)

2. **Configurar en `.env`:**
   ```bash
   LLM_PROVIDER=groq
   LLM_MODEL=llama-3.1-8b-instant
   GROQ_API_KEY=gsk_tu_key_aqui
   ```

3. **Reiniciar el backend:**
   ```bash
   pkill -f api_server.py
   python3 api_server.py
   ```

### Modelos disponibles en Groq (gratuitos):
- `llama-3.1-8b-instant` (muy r√°pido, recomendado)
- `mixtral-8x7b-32768` (m√°s potente)
- `gemma-7b-it` (alternativa)

## üéØ Opci√≥n 2: Hugging Face (Gratuita)

Hugging Face ofrece acceso gratuito a modelos de c√≥digo abierto.

### Pasos:

1. **Obtener API Key (Gratuita):**
   - Ve a https://huggingface.co/settings/tokens
   - Crea una cuenta gratuita
   - Genera un token (tipo "Read")
   - Copia el token (empieza con `hf_...`)

2. **Configurar en `.env`:**
   ```bash
   LLM_PROVIDER=huggingface
   LLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2
   HUGGINGFACE_API_KEY=hf_tu_token_aqui
   ```

3. **Reiniciar el backend**

### Modelos disponibles en Hugging Face:
- `mistralai/Mistral-7B-Instruct-v0.2`
- `meta-llama/Llama-2-7b-chat-hf`
- `google/gemma-7b-it`

## ‚öôÔ∏è Otras Opciones

### Ollama (Local - Requiere instalaci√≥n):
```bash
LLM_PROVIDER=ollama
LLM_MODEL=llama3
```
Requiere instalar Ollama localmente: https://ollama.ai/

### OpenRouter (Requiere API Key):
```bash
LLM_PROVIDER=openrouter
LLM_MODEL=openai/gpt-3.5-turbo
OPENROUTER_API_KEY=tu_key_aqui
```

## üìù Archivo `.env` de Ejemplo

Crea un archivo `.env` en la ra√≠z del proyecto:

```bash
# LLM Configuration (Groq - Recomendado)
LLM_PROVIDER=groq
LLM_MODEL=llama-3.1-8b-instant
GROQ_API_KEY=gsk_tu_key_aqui

# Alternativa: Hugging Face
# LLM_PROVIDER=huggingface
# LLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2
# HUGGINGFACE_API_KEY=hf_tu_token_aqui
```

## ‚úÖ Verificar Configuraci√≥n

Despu√©s de configurar, verifica que funciona:

```bash
curl http://localhost:5001/api/llm/status
```

Deber√≠as ver:
```json
{
  "provider": "groq",
  "model": "llama-3.1-8b-instant",
  "key_present": true,
  "available": true,
  "note": "Groq y Hugging Face son gratuitas. Groq es muy r√°pida."
}
```

## üéâ Ventajas de Groq

- ‚úÖ **Gratuita** (con l√≠mites generosos: 30 requests/minuto)
- ‚úÖ **Muy r√°pida** (respuestas en <1 segundo)
- ‚úÖ **No requiere instalaci√≥n local**
- ‚úÖ **Modelos modernos** (Llama 3.1, Mixtral, etc.)
- ‚úÖ **F√°cil de configurar** (solo necesitas una API key gratuita)

## üîß Soluci√≥n de Problemas

Si el chatbot no responde con LLM:

1. Verifica que tengas una API key configurada
2. Revisa los logs: `tail -f backend.log | grep -i llm`
3. Prueba cambiar a otro proveedor
4. Verifica la conexi√≥n a internet (necesaria para APIs)
5. Si no tienes API key, el chatbot usar√° respuestas predefinidas (m√°s r√°pidas pero menos inteligentes)

## üìö Recursos

- Groq: https://console.groq.com/
- Hugging Face: https://huggingface.co/
- Documentaci√≥n Groq: https://console.groq.com/docs
- Gu√≠a r√°pida: https://console.groq.com/quickstart

## üí° Nota Importante

**Sin API key configurada**, el chatbot seguir√° funcionando pero usar√° respuestas predefinidas basadas en la informaci√≥n del sitio. Para obtener respuestas m√°s inteligentes y contextuales, configura una API key gratuita de Groq (recomendado) o Hugging Face.
