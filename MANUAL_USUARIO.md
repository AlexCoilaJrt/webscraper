# üìñ Manual de Usuario - Web Scraper Inteligente

## üìã Tabla de Contenidos

1. [Introducci√≥n](#introducci√≥n)
2. [Instalaci√≥n y Configuraci√≥n](#instalaci√≥n-y-configuraci√≥n)
3. [Primeros Pasos](#primeros-pasos)
4. [Dashboard Principal](#dashboard-principal)
5. [Scraping Manual](#scraping-manual)
6. [Gesti√≥n de Art√≠culos](#gesti√≥n-de-art√≠culos)
7. [Galer√≠a de Im√°genes](#galer√≠a-de-im√°genes)
8. [Estad√≠sticas y An√°lisis](#estad√≠sticas-y-an√°lisis)
9. [Configuraci√≥n de Base de Datos](#configuraci√≥n-de-base-de-datos)
10. [Scraping Autom√°tico](#scraping-autom√°tico)
11. [Soluci√≥n de Problemas](#soluci√≥n-de-problemas)
12. [Referencia T√©cnica](#referencia-t√©cnica)

---

## üéØ Introducci√≥n

El **Web Scraper Inteligente** es un sistema completo para extraer, procesar y analizar contenido de sitios web de noticias. Con m√°s de **1,600 art√≠culos** extra√≠dos y **1,500 im√°genes** descargadas, el sistema ofrece:

- **An√°lisis inteligente** de p√°ginas web
- **Scraping autom√°tico** programado
- **Interfaz web moderna** con React y TypeScript
- **Gesti√≥n avanzada** de datos
- **Exportaci√≥n** a Excel
- **M√∫ltiples m√©todos** de extracci√≥n

---

## üõ†Ô∏è Instalaci√≥n y Configuraci√≥n

### Prerrequisitos

- **Python 3.11+**
- **Node.js 16+**
- **npm o yarn**
- **Git**

### 1. Clonar el Repositorio

```bash
git clone https://github.com/tu-usuario/web-scraper-inteligente.git
cd web-scraper-inteligente
```

### 2. Configurar Backend (Python)

```bash
# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

### 3. Configurar Frontend (React)

```bash
cd frontend
npm install
```

### 4. Inicializar Base de Datos

```bash
# El sistema crear√° autom√°ticamente la base de datos SQLite
python api_server.py
```

---

## üöÄ Primeros Pasos

### Iniciar el Sistema

#### Opci√≥n 1: Script Autom√°tico (Recomendado)
```bash
chmod +x start_app.sh
./start_app.sh
```

#### Opci√≥n 2: Inicio Manual

**Terminal 1 - Backend:**
```bash
python api_server.py
```
Servidor disponible en: `http://localhost:5001`

**Terminal 2 - Frontend:**
```bash
cd frontend
npm start
```
Aplicaci√≥n disponible en: `http://localhost:3000`

### Verificar Instalaci√≥n

1. Abre `http://localhost:3000` en tu navegador
2. Deber√≠as ver el Dashboard principal
3. Verifica que las estad√≠sticas se carguen correctamente

---

## üè† Dashboard Principal

### Descripci√≥n General

El Dashboard es el centro de control del sistema, mostrando:

- **M√©tricas en tiempo real** del sistema
- **Estado del scraping** actual
- **Gesti√≥n de peri√≥dicos** configurados
- **Acciones r√°pidas** para navegaci√≥n

### Componentes del Dashboard

#### üìä M√©tricas Principales

| M√©trica | Descripci√≥n | Valor Actual |
|---------|-------------|--------------|
| **Art√≠culos Extra√≠dos** | Total de art√≠culos en la base de datos | 1,600+ |
| **Im√°genes Descargadas** | Total de im√°genes almacenadas | 1,500+ |
| **Peri√≥dicos Monitoreados** | Fuentes de noticias activas | 10 |
| **Categor√≠as Identificadas** | Tipos de contenido clasificados | 5+ |

#### üîÑ Estado del Scraping

- **Sistema Activo**: Muestra cuando hay scraping en curso
- **Sistema Inactivo**: Estado de reposo, listo para iniciar
- **Progreso en Tiempo Real**: Barra de progreso y estad√≠sticas

#### üóûÔ∏è Gesti√≥n de Peri√≥dicos

Cada peri√≥dico muestra:
- **Nombre** del peri√≥dico
- **Cantidad de art√≠culos** extra√≠dos
- **Im√°genes descargadas**
- **Fecha de √∫ltima extracci√≥n**
- **Bot√≥n de eliminaci√≥n** selectiva

#### ‚ö° Acciones R√°pidas

- **Ver Art√≠culos**: Navegar a la lista de art√≠culos
- **Galer√≠a de Im√°genes**: Ver im√°genes descargadas
- **Ver Estad√≠sticas**: An√°lisis detallado
- **Configurar BD**: Configuraci√≥n de base de datos

### Funciones Especiales

#### üóëÔ∏è Limpieza de Datos

**Limpieza Total:**
- Elimina todos los art√≠culos
- Elimina todas las im√°genes
- Elimina todas las estad√≠sticas
- **‚ö†Ô∏è Acci√≥n irreversible**

**Eliminaci√≥n Selectiva:**
- Elimina datos de un peri√≥dico espec√≠fico
- Mantiene datos de otros peri√≥dicos
- **‚ö†Ô∏è Acci√≥n irreversible**

#### üîÑ Actualizaci√≥n Autom√°tica

- **Bot√≥n "ACTUALIZAR AUTOM√ÅTICO"**
- Ejecuta scraping de todos los peri√≥dicos configurados
- Actualiza datos en tiempo real
- Muestra progreso en el dashboard

---

## üîç Scraping Manual

### Acceso al Control de Scraping

1. Navega a la pesta√±a **"SCRAPING"** en el men√∫
2. O usa el bot√≥n **"INICIAR SCRAPING"** en el dashboard

### Configuraci√≥n de Scraping

#### üìù Campos de Configuraci√≥n

| Campo | Descripci√≥n | Ejemplo |
|-------|-------------|---------|
| **URL** | Direcci√≥n del sitio web a scrapear | `https://elcomercio.pe/` |
| **M√©todo** | T√©cnica de extracci√≥n | An√°lisis Inteligente |
| **M√°x. Art√≠culos** | L√≠mite de art√≠culos a extraer | 50 |
| **M√°x. Im√°genes** | L√≠mite de im√°genes por art√≠culo | 1 |
| **Categor√≠a** | Clasificaci√≥n del contenido | General |
| **Peri√≥dico** | Nombre de la fuente | El Comercio |
| **Regi√≥n** | Clasificaci√≥n geogr√°fica | Nacional |

#### üß† M√©todos de Scraping

##### 1. An√°lisis Inteligente (Recomendado)
- **Detecci√≥n autom√°tica** del mejor m√©todo
- **An√°lisis de p√°gina** completo
- **Recomendaci√≥n** con nivel de confianza
- **Detecci√≥n de idioma** y regi√≥n

**Caracter√≠sticas:**
- Analiza JavaScript y contenido din√°mico
- Detecta paginaci√≥n y lazy loading
- Clasifica autom√°ticamente la regi√≥n
- Proporciona razones para la recomendaci√≥n

##### 2. H√≠brido
- **Combina Requests y Selenium**
- **Ideal para sitios complejos**
- **Maneja contenido din√°mico**

**Cu√°ndo usar:**
- Sitios con mucho JavaScript
- Contenido que se carga asincr√≥nicamente
- SPAs (Single Page Applications)

##### 3. Optimizado
- **Paralelizaci√≥n** para m√°ximo rendimiento
- **M√∫ltiples workers** simult√°neos
- **Ideal para sitios est√°ticos**

**Cu√°ndo usar:**
- Sitios con muchos art√≠culos
- Contenido est√°tico
- Necesidad de velocidad m√°xima

##### 4. Mejorado
- **Sin Selenium** (menor uso de recursos)
- **Buena compatibilidad** general
- **Headers inteligentes**

**Cu√°ndo usar:**
- Sitios est√°ndar de noticias
- Cuando se necesita eficiencia de recursos
- Compatibilidad general

##### 5. Selenium
- **Navegador completo** con JavaScript
- **M√°xima compatibilidad**
- **Mayor uso de recursos**

**Cu√°ndo usar:**
- Sitios muy complejos
- SPAs complejas
- Cuando otros m√©todos fallan

### Proceso de Scraping

#### 1. An√°lisis de P√°gina (Solo con An√°lisis Inteligente)

El sistema analiza:
- **Tama√±o de p√°gina** y tiempo de respuesta
- **Presencia de JavaScript** y frameworks
- **Estructura de enlaces** y paginaci√≥n
- **Contenido din√°mico** y lazy loading
- **Idioma** y caracter√≠sticas regionales

#### 2. Extracci√≥n de Datos

- **B√∫squeda de enlaces** de art√≠culos
- **Extracci√≥n de contenido** de cada art√≠culo
- **Descarga de im√°genes** (si est√° habilitado)
- **Clasificaci√≥n** autom√°tica de categor√≠as

#### 3. Almacenamiento

- **Guardado en base de datos** SQLite
- **Prevenci√≥n de duplicados** autom√°tica
- **Actualizaci√≥n de estad√≠sticas**
- **Logging** detallado de operaciones

### Monitoreo en Tiempo Real

Durante el scraping, puedes ver:
- **Progreso actual** (art√≠culos procesados)
- **URL actual** siendo procesada
- **Art√≠culos encontrados** hasta el momento
- **Im√°genes descargadas**
- **Tiempo transcurrido**

---

## üì∞ Gesti√≥n de Art√≠culos

### Acceso a la Lista de Art√≠culos

1. Navega a la pesta√±a **"ART√çCULOS"** en el men√∫
2. O usa el bot√≥n **"Ver Art√≠culos"** en el dashboard

### Vista de Lista de Art√≠culos

#### üìã Informaci√≥n Mostrada

| Campo | Descripci√≥n |
|-------|-------------|
| **T√≠tulo** | T√≠tulo del art√≠culo |
| **Peri√≥dico** | Fuente de la noticia |
| **Categor√≠a** | Clasificaci√≥n del contenido |
| **Regi√≥n** | Nacional o Extranjero |
| **Fecha** | Fecha de extracci√≥n |
| **Im√°genes** | Cantidad de im√°genes |

#### üîç Filtros Disponibles

##### Filtro por Peri√≥dico
- **El Comercio** - 324 art√≠culos
- **La Vanguardia** - 150 art√≠culos
- **El Popular** - 129 art√≠culos
- **Trome** - 110 art√≠culos
- **Ojo** - 102 art√≠culos
- **Diario Sin Fronteras** - 66 art√≠culos
- **America** - 34 art√≠culos
- **Nytimes** - 27 art√≠culos
- **Peru21** - 18 art√≠culos
- **El Peruano** - 6 art√≠culos

##### Filtro por Categor√≠a
- **General** - Noticias generales
- **Internacional** - Noticias del extranjero
- **Regional** - Noticias regionales
- **Econom√≠a** - Noticias econ√≥micas

##### Filtro por Regi√≥n
- **Nacional** - Noticias de Per√∫
- **Extranjero** - Noticias internacionales

##### B√∫squeda de Texto
- **B√∫squeda en t√≠tulos**
- **B√∫squeda en contenido**
- **B√∫squeda en res√∫menes**

#### üìÑ Paginaci√≥n

- **20 art√≠culos por p√°gina** (configurable)
- **Navegaci√≥n** con botones anterior/siguiente
- **Informaci√≥n de paginaci√≥n** (p√°gina X de Y)
- **Total de art√≠culos** mostrado

### Funciones de Art√≠culos

#### üëÅÔ∏è Ver Art√≠culo Completo

Al hacer clic en un art√≠culo:
- **Vista detallada** del contenido completo
- **Informaci√≥n completa** (autor, fecha, etc.)
- **Im√°genes asociadas** (si las hay)
- **URL original** del art√≠culo

#### üìä Exportar a Excel

**Caracter√≠sticas de la exportaci√≥n:**
- **Formato profesional** con columnas ajustadas
- **Todos los art√≠culos** o filtrados
- **Informaci√≥n completa** (t√≠tulo, contenido, metadatos)
- **Nombre de archivo** con timestamp
- **Descarga autom√°tica** al navegador

**Columnas incluidas:**
- ID, T√≠tulo, Resumen, Contenido
- Peri√≥dico, Categor√≠a, Regi√≥n
- URL, Fecha de Extracci√≥n
- Cantidad de Im√°genes

---

## üñºÔ∏è Galer√≠a de Im√°genes

### Acceso a la Galer√≠a

1. Navega a la pesta√±a **"IM√ÅGENES"** en el men√∫
2. O usa el bot√≥n **"Galer√≠a de Im√°genes"** en el dashboard

### Vista de Galer√≠a

#### üñºÔ∏è Visualizaci√≥n de Im√°genes

- **Vista de cuadr√≠cula** con miniaturas
- **Vista previa** al pasar el mouse
- **Informaci√≥n de imagen** (tama√±o, formato, etc.)
- **Filtros** por peri√≥dico y fecha

#### üìã Informaci√≥n de Im√°genes

| Campo | Descripci√≥n |
|-------|-------------|
| **Archivo** | Nombre del archivo de imagen |
| **Peri√≥dico** | Fuente de la imagen |
| **Tama√±o** | Dimensiones en p√≠xeles |
| **Formato** | JPG, PNG, WebP, etc. |
| **Peso** | Tama√±o en bytes |
| **Fecha** | Fecha de descarga |

#### üîç Filtros de Im√°genes

- **Por peri√≥dico**: Filtrar im√°genes por fuente
- **Por fecha**: Rango de fechas de descarga
- **Por formato**: JPG, PNG, WebP, etc.
- **Por tama√±o**: Filtrar por dimensiones

#### ‚¨áÔ∏è Descarga de Im√°genes

- **Descarga individual**: Clic en imagen
- **Descarga m√∫ltiple**: Selecci√≥n m√∫ltiple
- **Informaci√≥n detallada**: Metadatos completos

---

## üìä Estad√≠sticas y An√°lisis

### Acceso a Estad√≠sticas

1. Navega a la pesta√±a **"ESTAD√çSTICAS"** en el men√∫
2. O usa el bot√≥n **"Ver Estad√≠sticas"** en el dashboard

### Tipos de Estad√≠sticas

#### üìà Estad√≠sticas Generales

- **Total de art√≠culos** extra√≠dos
- **Total de im√°genes** descargadas
- **Peri√≥dicos monitoreados**
- **Categor√≠as identificadas**
- **Regiones cubiertas**

#### üìä Estad√≠sticas por Peri√≥dico

**Top 10 Peri√≥dicos por Art√≠culos:**
1. **Elmundo** - 324 art√≠culos
2. **La Vanguardia** - 150 art√≠culos
3. **El Popular** - 129 art√≠culos
4. **Trome** - 110 art√≠culos
5. **Ojo** - 102 art√≠culos
6. **Diario Sin Fronteras** - 66 art√≠culos
7. **El Comercio** - 57 art√≠culos
8. **America** - 34 art√≠culos
9. **Dario Sin Fronteras** - 33 art√≠culos
10. **El popular** - 32 art√≠culos

#### üìã Estad√≠sticas por Categor√≠a

- **General** - Mayor cantidad de art√≠culos
- **Internacional** - Noticias del extranjero
- **Regional** - Noticias regionales
- **Econom√≠a** - Noticias econ√≥micas

#### ‚è±Ô∏è Sesiones de Scraping

**√öltimas 10 sesiones:**
- **Fecha y hora** de ejecuci√≥n
- **URL scrapeada**
- **Art√≠culos encontrados**
- **Im√°genes descargadas**
- **Duraci√≥n** de la sesi√≥n
- **M√©todo utilizado**

### Gr√°ficos Interactivos

#### üìä Gr√°fico de Barras - Art√≠culos por Peri√≥dico
- Visualizaci√≥n clara de la distribuci√≥n
- Comparaci√≥n entre peri√≥dicos
- Datos actualizados en tiempo real

#### ü•ß Gr√°fico Circular - Distribuci√≥n por Categor√≠a
- Porcentajes de cada categor√≠a
- Visualizaci√≥n intuitiva
- Colores diferenciados

#### üìà Gr√°fico de L√≠neas - Tendencias Temporales
- Evoluci√≥n del scraping en el tiempo
- Picos de actividad
- Tendencias de crecimiento

---

## ‚öôÔ∏è Configuraci√≥n de Base de Datos

### Acceso a Configuraci√≥n

1. Navega a la pesta√±a **"CONFIGURAR BD"** en el men√∫
2. O usa el bot√≥n **"Configurar BD"** en el dashboard

### Configuraciones Disponibles

#### üóÑÔ∏è Base de Datos Actual

**SQLite (Por defecto):**
- **Archivo**: `news_database.db`
- **Ubicaci√≥n**: Directorio ra√≠z del proyecto
- **Tama√±o**: ~50MB (con 1,600+ art√≠culos)
- **Tablas**: articles, images, scraping_stats

#### üîÑ Migraci√≥n a MySQL (Opcional)

**Configuraci√≥n MySQL:**
```python
DB_CONFIG = {
    'host': 'localhost',
    'port': 3306,
    'user': 'tu_usuario',
    'password': 'tu_contrase√±a',
    'database': 'noticias_db'
}
```

**Ventajas de MySQL:**
- **Mejor rendimiento** para grandes vol√∫menes
- **Concurrencia** mejorada
- **Respaldo** m√°s robusto
- **Escalabilidad** superior

#### üíæ Respaldo y Restauraci√≥n

**Respaldo de SQLite:**
```bash
cp news_database.db news_database_backup_$(date +%Y%m%d).db
```

**Respaldo de MySQL:**
```bash
mysqldump -u usuario -p noticias_db > backup_$(date +%Y%m%d).sql
```

### Gesti√≥n de Datos

#### üóëÔ∏è Limpieza de Base de Datos

**Limpieza Total:**
- Elimina todos los registros
- Resetea contadores
- Libera espacio en disco

**Limpieza Selectiva:**
- Elimina por per√≠odo de tiempo
- Elimina por peri√≥dico espec√≠fico
- Elimina art√≠culos duplicados

#### üìä Optimizaci√≥n de Base de Datos

**Comandos de optimizaci√≥n:**
```sql
-- SQLite
VACUUM;
ANALYZE;

-- MySQL
OPTIMIZE TABLE articles;
OPTIMIZE TABLE images;
OPTIMIZE TABLE scraping_stats;
```

---

## ü§ñ Scraping Autom√°tico

### Configuraci√≥n del Sistema Autom√°tico

#### üìÖ Programaci√≥n con Cron

**Configuraci√≥n actual:**
```bash
# Ejecutar cada 5 minutos
*/5 * * * * cd /ruta/al/proyecto && python auto_scraper_standalone.py
```

**Verificar configuraci√≥n:**
```bash
crontab -l
```

#### ‚öôÔ∏è Archivo de Configuraci√≥n

**Ubicaci√≥n**: `auto_scraping_config.json`

**Estructura:**
```json
{
  "auto_scraping": {
    "enabled": true,
    "schedules": [
      {
        "name": "El Comercio - Cada 5 minutos",
        "url": "https://elcomercio.pe/",
        "method": "auto",
        "max_articles": 50,
        "max_images": 1,
        "category": "General",
        "newspaper": "El Comercio",
        "region": "Nacional",
        "cron_schedule": "*/5 * * * *",
        "enabled": true
      }
    ]
  }
}
```

### Peri√≥dicos Configurados

#### üáµüá™ Peri√≥dicos Nacionales

| Peri√≥dico | URL | Art√≠culos/Max | Im√°genes/Max | Categor√≠a |
|-----------|-----|---------------|--------------|-----------|
| **El Comercio** | https://elcomercio.pe/ | 50 | 1 | General |
| **El Popular** | https://elpopular.pe/ | 40 | 1 | General |
| **Diario Sin Fronteras** | https://diariosinfronteras.com.pe/ | 35 | 1 | Regional |
| **El Peruano** | https://elperuano.pe/economia | 40 | 1 | Econom√≠a |
| **Peru21** | https://peru21.pe/ | 40 | 1 | General |
| **Ojo** | https://ojo.pe/ | 35 | 1 | General |
| **Trome** | https://trome.pe/ | 35 | 1 | General |

#### üåç Peri√≥dicos Internacionales

| Peri√≥dico | URL | Art√≠culos/Max | Im√°genes/Max | Categor√≠a |
|-----------|-----|---------------|--------------|-----------|
| **El Mundo** | https://www.elmundo.es/ | 50 | 1 | Internacional |
| **La Vanguardia** | https://www.lavanguardia.com/ | 50 | 1 | Internacional |
| **New York Times** | https://www.nytimes.com/ | 40 | 1 | Internacional |

### Ejecuci√≥n del Scraping Autom√°tico

#### üöÄ Inicio Manual

```bash
# Ejecutar scraping autom√°tico
python auto_scraper_standalone.py
```

#### üìä Monitoreo de Logs

**Archivo de log**: `auto_scraping.log`

**Comandos √∫tiles:**
```bash
# Ver logs en tiempo real
tail -f auto_scraping.log

# Ver √∫ltimas 50 l√≠neas
tail -n 50 auto_scraping.log

# Buscar errores
grep "ERROR" auto_scraping.log
```

#### üìà Estad√≠sticas de Ejecuci√≥n

**M√©tricas monitoreadas:**
- **Art√≠culos extra√≠dos** por sesi√≥n
- **Im√°genes descargadas**
- **Tiempo de ejecuci√≥n**
- **Errores encontrados**
- **Peri√≥dicos procesados**

---

## üîß Soluci√≥n de Problemas

### Problemas Comunes

#### ‚ùå Error: "Connection refused"

**Causa**: El servidor backend no est√° ejecut√°ndose

**Soluci√≥n**:
```bash
# Verificar que el backend est√© corriendo
curl http://localhost:5001/api/status

# Reiniciar el servidor
python api_server.py
```

#### ‚ùå Error: "ChromeDriver not found"

**Causa**: Driver de Chrome no disponible

**Soluci√≥n**:
```bash
# El sistema descarga autom√°ticamente el driver
# Si falla, instalar Chrome manualmente
brew install --cask google-chrome  # macOS
```

#### ‚ùå Error: "Module not found"

**Causa**: Dependencias Python no instaladas

**Soluci√≥n**:
```bash
# Reinstalar dependencias
pip install -r requirements.txt

# Verificar entorno virtual
source venv/bin/activate
```

#### ‚ùå Scraping autom√°tico no funciona

**Causa**: Cron no configurado o permisos incorrectos

**Soluci√≥n**:
```bash
# Verificar cron
crontab -l

# Verificar logs
tail -f auto_scraping.log

# Verificar permisos
chmod +x auto_scraper_standalone.py
```

#### ‚ùå Frontend no carga

**Causa**: Dependencias Node.js no instaladas

**Soluci√≥n**:
```bash
cd frontend
npm install
npm start
```

### Logs y Debugging

#### üìù Archivos de Log

| Archivo | Prop√≥sito |
|---------|-----------|
| `auto_scraping.log` | Logs del scraping autom√°tico |
| `api_server.log` | Logs del servidor API |
| `frontend.log` | Logs del frontend React |

#### üîç Comandos de Debugging

```bash
# Ver logs del sistema
journalctl -f

# Ver procesos Python
ps aux | grep python

# Ver puertos en uso
lsof -i :5001  # Backend
lsof -i :3000  # Frontend
```

### Optimizaci√≥n de Rendimiento

#### ‚ö° Mejoras de Velocidad

**Backend:**
- Usar m√©todo "Optimizado" para sitios est√°ticos
- Aumentar `max_workers` en SmartScraper
- Usar base de datos MySQL para grandes vol√∫menes

**Frontend:**
- Limitar art√≠culos por p√°gina (20-50)
- Usar filtros para reducir datos mostrados
- Habilitar compresi√≥n gzip

#### üíæ Optimizaci√≥n de Almacenamiento

**Base de datos:**
```sql
-- Limpiar registros antiguos
DELETE FROM articles WHERE scraped_at < '2024-01-01';

-- Optimizar base de datos
VACUUM;
```

**Im√°genes:**
```bash
# Comprimir im√°genes existentes
find scraped_images -name "*.jpg" -exec jpegoptim --max=80 {} \;
```

---

## üìö Referencia T√©cnica

### Arquitectura del Sistema

#### üèóÔ∏è Componentes Principales

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend      ‚îÇ    ‚îÇ   Backend       ‚îÇ    ‚îÇ   Database      ‚îÇ
‚îÇ   (React)       ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   (Flask)       ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   (SQLite)      ‚îÇ
‚îÇ   Port: 3000    ‚îÇ    ‚îÇ   Port: 5001    ‚îÇ    ‚îÇ   File: .db     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ                       ‚îÇ
         ‚îÇ                       ‚îÇ                       ‚îÇ
         ‚ñº                       ‚ñº                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Browser       ‚îÇ    ‚îÇ   Scrapers      ‚îÇ    ‚îÇ   Images        ‚îÇ
‚îÇ   (Chrome)      ‚îÇ    ‚îÇ   (5 Methods)   ‚îÇ    ‚îÇ   (Files)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### üîÑ Flujo de Datos

1. **Usuario** inicia scraping desde frontend
2. **Frontend** env√≠a request a backend API
3. **Backend** selecciona m√©todo de scraping
4. **Scraper** extrae datos del sitio web
5. **Backend** guarda datos en base de datos
6. **Frontend** actualiza interfaz con nuevos datos

### Librer√≠as y Dependencias

#### üêç Backend (Python)

| Librer√≠a | Versi√≥n | Prop√≥sito |
|----------|---------|-----------|
| **Flask** | 2.3.3 | Framework web REST API |
| **Flask-CORS** | 4.0.0 | CORS para frontend |
| **requests** | 2.31.0 | Cliente HTTP |
| **beautifulsoup4** | 4.12.2 | Parser HTML |
| **selenium** | 4.15.2 | Automatizaci√≥n navegador |
| **webdriver-manager** | 4.0.1 | Gesti√≥n drivers |
| **sqlalchemy** | 2.0.21 | ORM base de datos |
| **pandas** | 2.1.3 | Manipulaci√≥n datos |
| **openpyxl** | 3.1.2 | Exportaci√≥n Excel |
| **lxml** | 4.9.3 | Parser XML/HTML r√°pido |

#### ‚öõÔ∏è Frontend (React/TypeScript)

| Librer√≠a | Versi√≥n | Prop√≥sito |
|----------|---------|-----------|
| **React** | 19.1.1 | Framework UI |
| **TypeScript** | 4.9.5 | Tipado est√°tico |
| **@mui/material** | 7.3.2 | Componentes UI |
| **@mui/icons-material** | 7.3.2 | Iconos Material |
| **axios** | 1.12.1 | Cliente HTTP |
| **chart.js** | 4.5.0 | Gr√°ficos |
| **react-chartjs-2** | 5.3.0 | Integraci√≥n React-Chart |
| **react-router-dom** | 7.9.1 | Navegaci√≥n |
| **date-fns** | 4.1.0 | Manipulaci√≥n fechas |
| **xlsx** | 0.18.5 | Exportaci√≥n archivos |

### APIs y Endpoints

#### üîå Endpoints del Backend

| Endpoint | M√©todo | Descripci√≥n |
|----------|--------|-------------|
| `/api/health` | GET | Estado de la API |
| `/api/status` | GET | Estado del scraping |
| `/api/start-scraping` | POST | Iniciar scraping |
| `/api/stop-scraping` | POST | Detener scraping |
| `/api/articles` | GET | Listar art√≠culos |
| `/api/articles/export/excel` | GET | Exportar a Excel |
| `/api/images` | GET | Listar im√°genes |
| `/api/stats` | GET | Estad√≠sticas |
| `/api/newspapers` | GET | Listar peri√≥dicos |
| `/api/clear-all` | DELETE | Limpiar todos los datos |
| `/api/auto-update` | POST | Actualizaci√≥n autom√°tica |

### Base de Datos

#### üóÑÔ∏è Estructura de Tablas

**Tabla `articles`:**
```sql
CREATE TABLE articles (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    title TEXT NOT NULL,
    content TEXT,
    summary TEXT,
    author TEXT,
    date TEXT,
    category TEXT,
    newspaper TEXT,
    url TEXT NOT NULL,
    images_found INTEGER DEFAULT 0,
    images_downloaded INTEGER DEFAULT 0,
    images_data TEXT,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    article_id TEXT UNIQUE,
    region TEXT DEFAULT 'extranjero'
);
```

**Tabla `images`:**
```sql
CREATE TABLE images (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    article_id TEXT,
    url TEXT NOT NULL,
    local_path TEXT,
    alt_text TEXT,
    title TEXT,
    width INTEGER,
    height INTEGER,
    format TEXT,
    size_bytes INTEGER,
    relevance_score INTEGER DEFAULT 0,
    downloaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**Tabla `scraping_stats`:**
```sql
CREATE TABLE scraping_stats (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT,
    url_scraped TEXT,
    articles_found INTEGER,
    images_found INTEGER,
    images_downloaded INTEGER,
    duration_seconds INTEGER,
    method_used TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### M√©todos de Scraping Detallados

#### üß† An√°lisis Inteligente

**Algoritmo de an√°lisis:**
1. **An√°lisis de p√°gina** (tama√±o, tiempo respuesta)
2. **Detecci√≥n de JavaScript** (frameworks, librer√≠as)
3. **An√°lisis de estructura** (enlaces, paginaci√≥n)
4. **Detecci√≥n de contenido din√°mico**
5. **Clasificaci√≥n de idioma** y regi√≥n
6. **Recomendaci√≥n** con nivel de confianza

**Criterios de evaluaci√≥n:**
- **JavaScript pesado** ‚Üí Recomienda Selenium
- **Paginaci√≥n compleja** ‚Üí Recomienda H√≠brido
- **Contenido est√°tico** ‚Üí Recomienda Optimizado
- **Sitio est√°ndar** ‚Üí Recomienda Mejorado

#### üîÑ H√≠brido

**Estrategia:**
1. **Intento con Requests** (r√°pido)
2. **Fallback a Selenium** si falla
3. **Combinaci√≥n de resultados**
4. **Optimizaci√≥n** de tiempo total

#### ‚ö° Optimizado

**Paralelizaci√≥n:**
- **10 workers** simult√°neos
- **Pool de conexiones** HTTP
- **Procesamiento as√≠ncrono**
- **Gesti√≥n de memoria** optimizada

#### üõ†Ô∏è Mejorado

**Caracter√≠sticas:**
- **Headers inteligentes** (User-Agent, Accept, etc.)
- **Manejo de sesiones** persistente
- **Detecci√≥n de enlaces** mejorada
- **Filtrado de contenido** relevante

#### üåê Selenium

**Configuraci√≥n:**
- **Chrome headless** por defecto
- **WebDriver Manager** autom√°tico
- **Timeouts** configurables
- **Gesti√≥n de recursos** optimizada

---

## üìû Soporte y Contacto

### üÜò Obtener Ayuda

1. **Revisa** la secci√≥n de soluci√≥n de problemas
2. **Consulta** los logs del sistema
3. **Verifica** la configuraci√≥n
4. **Contacta** al desarrollador

### üìß Informaci√≥n de Contacto

- **Desarrollador**: Tu Nombre
- **Email**: tu-email@ejemplo.com
- **GitHub**: [@tu-usuario](https://github.com/tu-usuario)

### üîÑ Actualizaciones

- **Versi√≥n actual**: 2.0
- **√öltima actualizaci√≥n**: Diciembre 2024
- **Pr√≥ximas caracter√≠sticas**: 
  - Soporte para m√°s peri√≥dicos
  - An√°lisis de sentimientos
  - API REST p√∫blica
  - Dashboard m√≥vil

---

## üìÑ Licencia

Este proyecto est√° bajo la **Licencia MIT**. Ver el archivo `LICENSE` para m√°s detalles.

---

## üôè Agradecimientos

- **BeautifulSoup** por el parsing HTML
- **Selenium** por la automatizaci√≥n de navegador
- **Material-UI** por los componentes React
- **Chart.js** por las visualizaciones
- **Flask** por el framework web
- **React** por el framework frontend

---

‚≠ê **¬°Si te gusta este proyecto, no olvides darle una estrella en GitHub!** ‚≠ê

