# ğŸ•·ï¸ Web Scraper Inteligente

Un sistema completo de web scraping con anÃ¡lisis inteligente, paginaciÃ³n automÃ¡tica y exportaciÃ³n de datos. Extrae artÃ­culos de mÃºltiples periÃ³dicos y los almacena en una base de datos SQLite con interfaz web moderna.

## ğŸ“Š EstadÃ­sticas del Proyecto

- **ğŸ“° Total de artÃ­culos extraÃ­dos:** 1,088
- **ğŸ–¼ï¸ Total de imÃ¡genes descargadas:** 119
- **ğŸ“ˆ Sesiones de scraping:** 74
- **ğŸ—ï¸ PeriÃ³dicos configurados:** 13
- **ğŸ¤– Sistema de scraping automÃ¡tico:** Activo (cada 5 horas)

## ğŸ—ï¸ PeriÃ³dicos Soportados

| PeriÃ³dico | ArtÃ­culos | RegiÃ³n | CategorÃ­a |
|-----------|-----------|--------|-----------|
| **Elmundo** | 324 | Extranjero | Internacional |
| **La Vanguardia** | 150 | Extranjero | Internacional |
| **El Popular** | 129 | Nacional | General |
| **Trome** | 110 | Nacional | General |
| **Ojo** | 102 | Nacional | General |
| **Diario Sin Fronteras** | 66 | Nacional | Regional |
| **El Comercio** | 57 | Nacional | General |
| **America** | 34 | Nacional | General |
| **Dario Sin Fronteras** | 33 | Nacional | Regional |
| **El popular** | 32 | Nacional | General |
| **Nytimes** | 27 | Extranjero | Internacional |
| **Peru21** | 18 | Nacional | General |
| **El Peruano** | 6 | Nacional | EconomÃ­a |

## âœ¨ CaracterÃ­sticas Principales

### ğŸ§  AnÃ¡lisis Inteligente
- **DetecciÃ³n automÃ¡tica** del mejor mÃ©todo de scraping
- **AnÃ¡lisis de pÃ¡gina** (JavaScript, SPA, paginaciÃ³n, lazy loading)
- **RecomendaciÃ³n inteligente** con nivel de confianza

### ğŸ”„ Scraping AutomÃ¡tico
- **PaginaciÃ³n automÃ¡tica** para extraer todos los artÃ­culos
- **Sistema de cron** configurado para ejecutar cada 5 horas
- **MÃºltiples mÃ©todos** de scraping (Hybrid, Optimized, Improved, Selenium)

### ğŸ“Š GestiÃ³n de Datos
- **Base de datos SQLite** para almacenamiento local
- **ExportaciÃ³n a Excel** con formato profesional
- **Filtros avanzados** por periÃ³dico, categorÃ­a y regiÃ³n
- **BÃºsqueda de texto** en tÃ­tulos y contenido

### ğŸ¨ Interfaz Moderna
- **Frontend React** con Material-UI
- **Dashboard profesional** con estadÃ­sticas en tiempo real
- **GalerÃ­a de imÃ¡genes** con vista previa
- **GrÃ¡ficos interactivos** para anÃ¡lisis de datos

## ğŸ› ï¸ TecnologÃ­as Utilizadas

### Backend
- **Python 3.11+**
- **Flask** - Framework web
- **SQLite** - Base de datos
- **Selenium** - AutomatizaciÃ³n de navegador
- **BeautifulSoup** - Parsing HTML
- **Requests** - Cliente HTTP
- **Pandas** - ManipulaciÃ³n de datos
- **OpenPyXL** - ExportaciÃ³n Excel

### Frontend
- **React 18**
- **TypeScript**
- **Material-UI** - Componentes UI
- **Chart.js** - GrÃ¡ficos
- **Axios** - Cliente HTTP

### Herramientas
- **WebDriver Manager** - GestiÃ³n automÃ¡tica de drivers
- **Cron** - ProgramaciÃ³n de tareas
- **MySQL Connector** - ConexiÃ³n MySQL (opcional)

## ğŸ“¦ InstalaciÃ³n

### Prerrequisitos
- Python 3.11 o superior
- Node.js 16 o superior
- npm o yarn
- Git

### 1. Clonar el Repositorio
```bash
git clone https://github.com/tu-usuario/web-scraper-inteligente.git
cd web-scraper-inteligente
```

### 2. Configurar Backend (Python)
```bash
# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

### 3. Configurar Frontend (React)
```bash
cd frontend
npm install
```

### 4. Inicializar Base de Datos
```bash
# El sistema crearÃ¡ automÃ¡ticamente la base de datos SQLite
python api_server.py
```

## ğŸš€ Uso

### Iniciar el Sistema

#### 1. Backend (Terminal 1)
```bash
python api_server.py
```
El servidor se ejecutarÃ¡ en `http://localhost:5001`

#### 2. Frontend (Terminal 2)
```bash
cd frontend
npm start
```
La aplicaciÃ³n se abrirÃ¡ en `http://localhost:3000`

### Configurar Scraping AutomÃ¡tico

#### 1. Activar Cron (macOS/Linux)
```bash
# El sistema ya estÃ¡ configurado para ejecutarse cada 5 horas
crontab -l  # Verificar configuraciÃ³n
```

#### 2. Configurar PeriÃ³dicos
Edita `auto_scraping_config.json` para agregar/modificar periÃ³dicos:

```json
{
  "auto_scraping": {
    "enabled": true,
    "schedules": [
      {
        "name": "El Comercio - Cada 5 horas",
        "url": "https://elcomercio.pe/",
        "method": "auto",
        "max_articles": 30,
        "max_images": 15,
        "category": "General",
        "newspaper": "El Comercio",
        "region": "Nacional",
        "cron_schedule": "0 */5 * * *",
        "enabled": true
      }
    ]
  }
}
```

## ğŸ“– Manual de Usuario

### ğŸ  Dashboard Principal
- **Vista general** de estadÃ­sticas
- **GestiÃ³n de periÃ³dicos** con opciones de eliminaciÃ³n
- **BotÃ³n de limpieza** de todos los datos
- **Estado del sistema** en tiempo real

### ğŸ” Scraping Manual
1. Ve a la pestaÃ±a **"SCRAPING"**
2. Ingresa la **URL** del sitio web
3. Selecciona el **mÃ©todo** (recomendado: "AnÃ¡lisis Inteligente")
4. Configura **parÃ¡metros** (artÃ­culos, imÃ¡genes, categorÃ­a, etc.)
5. Haz clic en **"INICIAR SCRAPING"**

### ğŸ“° GestiÃ³n de ArtÃ­culos
1. Ve a la pestaÃ±a **"ARTÃCULOS"**
2. **Filtra** por periÃ³dico, categorÃ­a o regiÃ³n
3. **Busca** en tÃ­tulos y contenido
4. **Exporta** a Excel con un clic
5. **Visualiza** artÃ­culos individuales

### ğŸ–¼ï¸ GalerÃ­a de ImÃ¡genes
1. Ve a la pestaÃ±a **"IMÃGENES"**
2. **Navega** por todas las imÃ¡genes descargadas
3. **Filtra** por periÃ³dico o fecha
4. **Descarga** imÃ¡genes individuales

### ğŸ“Š EstadÃ­sticas
1. Ve a la pestaÃ±a **"ESTADÃSTICAS"**
2. **Visualiza** grÃ¡ficos de rendimiento
3. **Analiza** tendencias temporales
4. **Revisa** sesiones de scraping

## âš™ï¸ ConfiguraciÃ³n Avanzada

### MÃ©todos de Scraping

#### ğŸ§  AnÃ¡lisis Inteligente (Recomendado)
- Detecta automÃ¡ticamente el mejor mÃ©todo
- Analiza caracterÃ­sticas de la pÃ¡gina
- Proporciona recomendaciÃ³n con confianza

#### ğŸ”„ HÃ­brido
- Combina Requests y Selenium
- Ideal para sitios con JavaScript
- Maneja contenido dinÃ¡mico

#### âš¡ Optimizado
- Usa paralelizaciÃ³n
- MÃ¡s rÃ¡pido para sitios estÃ¡ticos
- Ideal para sitios con muchos artÃ­culos

#### ğŸ› ï¸ Mejorado
- MÃ©todo robusto sin Selenium
- Buena compatibilidad
- Menor uso de recursos

#### ğŸŒ Selenium
- Navegador completo
- Para sitios muy complejos
- Mayor uso de recursos

### ConfiguraciÃ³n de Base de Datos

#### SQLite (Por defecto)
```python
DB_PATH = "news_database.db"
```

#### MySQL (Opcional)
```python
DB_PATH = "mysql://usuario:contraseÃ±a@localhost:3306/noticias_db"
```

## ğŸ”§ SoluciÃ³n de Problemas

### Error: "Connection refused"
```bash
# Verificar que el backend estÃ© corriendo
curl http://localhost:5001/api/status
```

### Error: "ChromeDriver not found"
```bash
# El sistema descarga automÃ¡ticamente el driver
# Si falla, instalar Chrome manualmente
```

### Error: "Module not found"
```bash
# Reinstalar dependencias
pip install -r requirements.txt
```

### Scraping automÃ¡tico no funciona
```bash
# Verificar cron
crontab -l

# Verificar logs
tail -f auto_scraping.log
```

## ğŸ“ Estructura del Proyecto

```
web-scraper-inteligente/
â”œâ”€â”€ ğŸ“ frontend/                 # AplicaciÃ³n React
â”‚   â”œâ”€â”€ ğŸ“ src/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ components/       # Componentes reutilizables
â”‚   â”‚   â”œâ”€â”€ ğŸ“ pages/           # PÃ¡ginas principales
â”‚   â”‚   â””â”€â”€ ğŸ“ services/        # Servicios API
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ ğŸ“ images/                  # ImÃ¡genes descargadas
â”œâ”€â”€ ğŸ“„ api_server.py           # Servidor Flask principal
â”œâ”€â”€ ğŸ“„ hybrid_crawler.py       # Scraper hÃ­brido
â”œâ”€â”€ ğŸ“„ optimized_scraper.py    # Scraper optimizado
â”œâ”€â”€ ğŸ“„ improved_scraper.py     # Scraper mejorado
â”œâ”€â”€ ğŸ“„ intelligent_analyzer.py # Analizador inteligente
â”œâ”€â”€ ğŸ“„ pagination_crawler.py   # Crawler de paginaciÃ³n
â”œâ”€â”€ ğŸ“„ auto_scraper_standalone.py # Scraper automÃ¡tico
â”œâ”€â”€ ğŸ“„ auto_scraping_config.json # ConfiguraciÃ³n automÃ¡tica
â”œâ”€â”€ ğŸ“„ news_database.db        # Base de datos SQLite
â”œâ”€â”€ ğŸ“„ requirements.txt        # Dependencias Python
â””â”€â”€ ğŸ“„ README.md              # Este archivo
```

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo `LICENSE` para mÃ¡s detalles.

## ğŸ‘¨â€ğŸ’» Autor

**Tu Nombre**
- GitHub: [@tu-usuario](https://github.com/tu-usuario)
- Email: tu-email@ejemplo.com

## ğŸ™ Agradecimientos

- **BeautifulSoup** por el parsing HTML
- **Selenium** por la automatizaciÃ³n de navegador
- **Material-UI** por los componentes React
- **Chart.js** por las visualizaciones
- **Flask** por el framework web

---

## ğŸ“ Soporte

Si tienes problemas o preguntas:

1. **Revisa** la secciÃ³n de soluciÃ³n de problemas
2. **Consulta** los issues existentes en GitHub
3. **Crea** un nuevo issue con detalles del problema
4. **Contacta** al autor por email

---

â­ **Â¡Si te gusta este proyecto, no olvides darle una estrella en GitHub!** â­