# Investigaci√≥n: Scraping de Reddit - Mejores Pr√°cticas 2024-2025

## üìã Resumen Ejecutivo

Reddit es una plataforma de comunidades (subreddits) donde los usuarios publican contenido y comentarios. A diferencia de Facebook y Twitter/X, Reddit tiene una **API oficial muy robusta** (PRAW) que es la forma recomendada y m√°s √©tica de extraer datos.

## üéØ M√©todos de Extracci√≥n

### 1. **API Oficial de Reddit (RECOMENDADO) - PRAW**

**Ventajas:**
- ‚úÖ Legal y √©tico
- ‚úÖ Estructurado y confiable
- ‚úÖ No requiere Selenium
- ‚úÖ Respeta l√≠mites de tasa autom√°ticamente
- ‚úÖ Acceso a metadatos completos

**Desventajas:**
- ‚ö†Ô∏è Requiere registro de aplicaci√≥n
- ‚ö†Ô∏è Requiere autenticaci√≥n (client_id, client_secret)
- ‚ö†Ô∏è Tiene l√≠mites de tasa (60 requests/minuto)

**Instalaci√≥n:**
```bash
pip install praw
```

**Configuraci√≥n:**
1. Ir a https://www.reddit.com/prefs/apps
2. Crear aplicaci√≥n (tipo: "script")
3. Obtener `client_id` y `client_secret`
4. Definir `user_agent` (ej: "MyApp/1.0 by MyUsername")

**Ejemplo de Uso:**
```python
import praw

reddit = praw.Reddit(
    client_id='TU_CLIENT_ID',
    client_secret='TU_CLIENT_SECRET',
    user_agent='MyApp/1.0 by MyUsername'
)

# Obtener posts de un subreddit
subreddit = reddit.subreddit('python')
for post in subreddit.hot(limit=10):
    print(f"T√≠tulo: {post.title}")
    print(f"Score: {post.score}")
    print(f"URL: {post.url}")
    print(f"Comentarios: {post.num_comments}")
```

### 2. **Web Scraping con Selenium (ALTERNATIVO)**

**Cu√°ndo usar:**
- No tienes acceso a API
- Necesitas datos que la API no proporciona
- Solo para fines acad√©micos

**Consideraciones:**
- ‚ö†Ô∏è Reddit ha implementado medidas anti-scraping
- ‚ö†Ô∏è Puede violar t√©rminos de servicio
- ‚ö†Ô∏è Menos confiable que la API
- ‚ö†Ô∏è Requiere m√°s recursos (Selenium)

**Selectores CSS (2025):**
```css
/* Posts principales */
shreddit-post, article[data-testid="post-container"]
div[data-testid="post-container"]
div[class*="Post"]

/* Contenedor de post */
div[data-testid="post-container"]

/* T√≠tulo del post */
h3[data-testid="post-title"], 
a[data-click-id="body"]

/* Texto del post */
div[data-test-id="post-content"] p,
div[slot="text-body"]

/* Upvotes/Downvotes */
button[aria-label*="upvote"],
button[aria-label*="downvote"],
span[data-testid="vote-count"]

/* Comentarios */
shreddit-comment,
div[data-testid="comment"]

/* URL del post */
a[data-click-id="body"]

/* Autor */
a[data-testid="subreddit-name"],
a[data-testid="author-name"]

/* Fecha */
time[datetime],
span[data-testid="post_timestamp"]
```

**Estructura HTML de Reddit:**
```html
<shreddit-post>
  <article data-testid="post-container">
    <div class="Post">
      <h3 data-testid="post-title">T√≠tulo del post</h3>
      <div slot="text-body">Contenido del post</div>
      <div class="PostFooter">
        <button aria-label="upvote">‚Üë</button>
        <span data-testid="vote-count">1.2k</span>
        <a data-click-id="comments">Comentarios</a>
      </div>
    </div>
  </article>
</shreddit-post>
```

### 3. **Reddit Old (old.reddit.com) - M√ÅS F√ÅCIL DE SCRAPEAR**

**Ventajas:**
- ‚úÖ HTML m√°s simple y estable
- ‚úÖ Menos JavaScript din√°mico
- ‚úÖ M√°s f√°cil de scrapear con BeautifulSoup

**Estructura:**
```python
# URL: https://old.reddit.com/r/subreddit/
# Selectores:
div.thing  # Contenedor de post
p.title > a.title  # T√≠tulo del post
div.score  # Puntos (upvotes)
a.comments  # Link a comentarios
span.tagline  # Autor y fecha
```

## üìä Datos que se Pueden Extraer

### Posts:
- T√≠tulo
- Contenido/Texto
- Autor (username)
- Subreddit
- Upvotes/Downvotes (score)
- N√∫mero de comentarios
- URL del post
- Fecha de publicaci√≥n
- Im√°genes/Videos (si aplica)
- Flair (etiquetas)

### Comentarios:
- Texto del comentario
- Autor
- Upvotes
- Fecha
- Respuestas (threads)

## üîß Implementaci√≥n Recomendada

### Arquitectura H√≠brida:

1. **Intentar API primero (PRAW)**
   - Si hay credenciales disponibles
   - M√°s confiable y r√°pido

2. **Fallback a Selenium**
   - Si no hay API disponible
   - Para contenido din√°mico
   - Usar old.reddit.com si es posible

### M√©todos de Extracci√≥n:

**M√©todo 1: API con PRAW (PREFERIDO)**
```python
import praw

class RedditAPIScraper:
    def __init__(self, client_id, client_secret, user_agent):
        self.reddit = praw.Reddit(
            client_id=client_id,
            client_secret=client_secret,
            user_agent=user_agent
        )
    
    def get_subreddit_posts(self, subreddit_name, limit=100, sort='hot'):
        subreddit = self.reddit.subreddit(subreddit_name)
        
        if sort == 'hot':
            posts = subreddit.hot(limit=limit)
        elif sort == 'new':
            posts = subreddit.new(limit=limit)
        elif sort == 'top':
            posts = subreddit.top(limit=limit)
        
        results = []
        for post in posts:
            results.append({
                'title': post.title,
                'content': post.selftext,
                'author': str(post.author),
                'subreddit': subreddit_name,
                'score': post.score,
                'upvotes': post.ups,
                'downvotes': post.downs,
                'comments': post.num_comments,
                'url': post.url,
                'permalink': f"https://reddit.com{post.permalink}",
                'created_at': datetime.fromtimestamp(post.created_utc),
                'flair': post.link_flair_text,
                'image_url': post.url if post.url.endswith(('.jpg', '.png', '.gif')) else None
            })
        
        return results
```

**M√©todo 2: Selenium (Fallback)**
```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class RedditSeleniumScraper:
    def __init__(self, headless=True):
        self.driver = self._setup_driver(headless)
    
    def scrape_subreddit(self, subreddit_name, max_posts=100):
        # Usar old.reddit.com para m√°s facilidad
        url = f"https://old.reddit.com/r/{subreddit_name}/"
        self.driver.get(url)
        
        posts = []
        scrolls = 0
        max_scrolls = 50
        
        while len(posts) < max_posts and scrolls < max_scrolls:
            # Extraer posts visibles
            post_elements = self.driver.find_elements(By.CSS_SELECTOR, 'div.thing')
            
            for post_elem in post_elements:
                try:
                    title = post_elem.find_element(By.CSS_SELECTOR, 'p.title > a.title').text
                    author = post_elem.find_element(By.CSS_SELECTOR, 'a.author').text
                    score = post_elem.find_element(By.CSS_SELECTOR, 'div.score').text
                    comments = post_elem.find_element(By.CSS_SELECTOR, 'a.comments').text
                    post_url = post_elem.find_element(By.CSS_SELECTOR, 'p.title > a.title').get_attribute('href')
                    
                    posts.append({
                        'title': title,
                        'author': author,
                        'score': self._parse_score(score),
                        'comments': self._parse_comments(comments),
                        'url': post_url,
                        'subreddit': subreddit_name
                    })
                except:
                    continue
            
            # Scroll para cargar m√°s
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            scrolls += 1
        
        return posts[:max_posts]
```

## ‚ö†Ô∏è Consideraciones Legales y √âticas

### IMPORTANTE:
1. **Reddit proh√≠be scraping no autorizado** en sus T√©rminos de Servicio
2. **Reddit ha demandado empresas** por scraping no autorizado (ej: Anthropic, Perplexity)
3. **Usa la API oficial** siempre que sea posible
4. **Respeta robots.txt**: Reddit actualiz√≥ su robots.txt en 2024 para bloquear scraping

### Mejores Pr√°cticas:
- ‚úÖ Usar API oficial (PRAW) cuando sea posible
- ‚úÖ Respetar l√≠mites de tasa (60 requests/minuto)
- ‚úÖ Usar delays responsables (1-2 segundos entre requests)
- ‚úÖ Solo extraer datos p√∫blicos
- ‚úÖ No almacenar informaci√≥n personal
- ‚úÖ Solo para fines acad√©micos/educativos
- ‚ö†Ô∏è Evitar scraping automatizado del sitio web principal

## üîç Selectores CSS para Reddit (2025)

### Reddit Moderno (reddit.com):
```css
/* Posts */
shreddit-post
article[data-testid="post-container"]
div[data-testid="post-container"]

/* T√≠tulo */
h3[data-testid="post-title"]
a[data-click-id="body"]

/* Contenido */
div[slot="text-body"]
div[data-test-id="post-content"]

/* Upvotes */
button[aria-label*="upvote"]
span[data-testid="vote-count"]

/* Autor */
a[data-testid="author-name"]
a[data-testid="subreddit-name"]

/* Comentarios */
a[data-click-id="comments"]
span[data-testid="comment-count"]
```

### Reddit Old (old.reddit.com):
```css
/* Posts */
div.thing

/* T√≠tulo */
p.title > a.title

/* Autor */
a.author

/* Score */
div.score

/* Comentarios */
a.comments

/* Fecha */
time.live-timestamp
```

## üìù Formato de Datos Esperado

```python
{
    'platform': 'reddit',
    'title': 'T√≠tulo del post',
    'content': 'Contenido del post',
    'author': 'username',
    'subreddit': 'subreddit_name',
    'score': 1234,  # Upvotes - downvotes
    'upvotes': 1500,
    'downvotes': 266,
    'comments': 89,
    'url': 'https://reddit.com/r/...',
    'permalink': 'https://reddit.com/r/.../comments/...',
    'created_at': '2025-01-15T10:30:00',
    'flair': 'Discussion',  # Etiqueta del post
    'image_url': 'https://...',  # Si tiene imagen
    'category': 'tecnologia',  # Categor√≠a detectada
    'sentiment': 'positive',  # Sentimiento
    'hashtags': []  # Reddit no usa hashtags tradicionalmente
}
```

## üöÄ Plan de Implementaci√≥n

1. **Crear RedditAPIScraper** (PRAW)
2. **Crear RedditSeleniumScraper** (Fallback)
3. **Integrar en social_media_scraper.py**
4. **Actualizar api_server.py** para soportar Reddit
5. **Actualizar frontend** para mostrar Reddit como opci√≥n
6. **Agregar Reddit a requirements.txt** (praw)

## üìö Recursos Adicionales

- [PRAW Documentation](https://praw.readthedocs.io/)
- [Reddit API Documentation](https://www.reddit.com/dev/api/)
- [Reddit Data API Terms](https://www.redditinc.com/policies/data-api-terms)
- [Pushshift API](https://github.com/pushshift/api) - Para datos hist√≥ricos

## ‚ö° Ventajas de Reddit vs Facebook/Twitter

1. **API Oficial Robusta**: PRAW es muy completo
2. **Datos Estructurados**: Posts, comentarios, subreddits bien organizados
3. **Menos Anti-Detecci√≥n**: Si usas API, no hay problemas
4. **Datos M√°s Ricos**: Upvotes, downvotes, flairs, etc.

## üéØ Recomendaci√≥n Final

**Para este proyecto acad√©mico:**
1. Implementar RedditAPIScraper con PRAW (m√©todo principal)
2. Implementar RedditSeleniumScraper como fallback (old.reddit.com)
3. Priorizar API, usar Selenium solo si no hay credenciales
4. Documentar claramente que es solo para fines acad√©micos















