#!/usr/bin/env python3
"""
Scraper autom√°tico - Ejecuta scraping programado
"""

import json
import requests
import logging
from datetime import datetime
import time

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('auto_scraping.log'),
        logging.StreamHandler()
    ]
)

def load_config():
    """Cargar configuraci√≥n de scraping autom√°tico"""
    try:
        with open('auto_scraping_config.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logging.error("‚ùå Archivo de configuraci√≥n no encontrado")
        return None

def execute_scraping(schedule):
    """Ejecutar scraping para una programaci√≥n espec√≠fica"""
    try:
        logging.info(f"üöÄ Iniciando scraping: {schedule['name']}")
        
        # Preparar datos para la API
        data = {
            "url": schedule["url"],
            "max_articles": schedule["max_articles"],
            "max_images": schedule["max_images"],
            "method": schedule["method"],
            "download_images": True,
            "category": schedule["category"],
            "newspaper": schedule["newspaper"],
            "region": schedule["region"]
        }
        
        # Llamar a la API
        response = requests.post(
            "http://localhost:5001/api/start-scraping",
            json=data,
            timeout=300  # 5 minutos timeout
        )
        
        if response.status_code == 200:
            result = response.json()
            logging.info(f"‚úÖ Scraping completado: {schedule['name']}")
            logging.info(f"   - Art√≠culos: {result.get('status', {}).get('articles_found', 0)}")
            logging.info(f"   - Im√°genes: {result.get('status', {}).get('images_found', 0)}")
            return True
        else:
            logging.error(f"‚ùå Error en scraping {schedule['name']}: {response.status_code}")
            return False
            
    except Exception as e:
        logging.error(f"‚ùå Error ejecutando scraping {schedule['name']}: {e}")
        return False

def main():
    """Funci√≥n principal"""
    logging.info("üïê Iniciando scraping autom√°tico")
    
    # Cargar configuraci√≥n
    config = load_config()
    if not config:
        return
    
    auto_config = config.get("auto_scraping", {})
    if not auto_config.get("enabled", False):
        logging.info("‚è∏Ô∏è Scraping autom√°tico deshabilitado")
        return
    
    # Ejecutar cada programaci√≥n
    schedules = auto_config.get("schedules", [])
    successful = 0
    failed = 0
    
    for schedule in schedules:
        if schedule.get("enabled", False):
            if execute_scraping(schedule):
                successful += 1
            else:
                failed += 1
            
            # Esperar entre ejecuciones para no sobrecargar
            time.sleep(30)
    
    logging.info(f"üìä Resumen: {successful} exitosos, {failed} fallidos")
    logging.info("üèÅ Scraping autom√°tico completado")

if __name__ == "__main__":
    main()
