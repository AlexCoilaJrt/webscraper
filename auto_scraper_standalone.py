#!/usr/bin/env python3
"""
Scraper autom√°tico independiente - No requiere servidor API
"""

import json
import logging
import time
from datetime import datetime
import sys
import os

# Agregar el directorio actual al path para importar los m√≥dulos
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from hybrid_crawler import HybridDataCrawler
from optimized_scraper import SmartScraper

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('auto_scraping.log'),
        logging.StreamHandler()
    ]
)

def load_config():
    """Cargar configuraci√≥n de scraping autom√°tico"""
    try:
        with open('auto_scraping_config.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logging.error("‚ùå Archivo de configuraci√≥n no encontrado")
        return None

def save_articles_to_db(articles, category='', newspaper='', region=''):
    """Guardar art√≠culos en la base de datos SQLite"""
    try:
        import sqlite3
        from datetime import datetime
        
        conn = sqlite3.connect('scraping_data.db')
        cursor = conn.cursor()
        
        for article in articles:
            # Verificar si el art√≠culo ya existe
            cursor.execute("SELECT id FROM articles WHERE url = ?", (article.get('url', ''),))
            if cursor.fetchone():
                continue  # Saltar si ya existe
            
            # Insertar nuevo art√≠culo
            cursor.execute("""
                INSERT INTO articles (
                    title, content, url, summary, author, published_date, 
                    scraped_at, category, newspaper, region, images_found, 
                    images_downloaded, images_data
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                article.get('title', ''),
                article.get('content', ''),
                article.get('url', ''),
                article.get('summary', ''),
                article.get('author', ''),
                article.get('published_date', ''),
                datetime.now().isoformat(),
                category,
                newspaper,
                region,
                article.get('images_found', 0),
                article.get('images_downloaded', 0),
                json.dumps(article.get('images_data', []))
            ))
        
        conn.commit()
        conn.close()
        return True
        
    except Exception as e:
        logging.error(f"‚ùå Error guardando en base de datos: {e}")
        return False

def execute_scraping_standalone(schedule):
    """Ejecutar scraping independiente (sin API)"""
    try:
        logging.info(f"üöÄ Iniciando scraping: {schedule['name']}")
        
        url = schedule["url"]
        method = schedule["method"]
        max_articles = schedule["max_articles"]
        max_images = schedule["max_images"]
        category = schedule["category"]
        newspaper = schedule["newspaper"]
        region = schedule["region"]
        
        articles = []
        
        if method == "hybrid":
            # Usar HybridDataCrawler
            crawler = HybridDataCrawler()
            try:
                articles = crawler.hybrid_crawl_articles(url, max_articles)
                logging.info(f"‚úÖ HybridDataCrawler: {len(articles)} art√≠culos encontrados")
            finally:
                crawler.close()
                
        elif method == "optimized":
            # Usar SmartScraper
            scraper = SmartScraper(max_workers=10)
            try:
                articles = scraper.crawl_and_scrape_parallel(url, max_articles=max_articles, extract_images=True)
                logging.info(f"‚úÖ SmartScraper: {len(articles)} art√≠culos encontrados")
            finally:
                scraper.close()
                
        elif method == "webscraping":
            # Usar m√©todo b√°sico con requests
            import requests
            from bs4 import BeautifulSoup
            
            try:
                response = requests.get(url, timeout=30)
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Extraer enlaces b√°sicos
                links = []
                for link in soup.find_all('a', href=True):
                    href = link.get('href')
                    if href and len(href) > 20:
                        links.append(href)
                
                # Crear art√≠culos b√°sicos
                articles = []
                for i, link in enumerate(links[:max_articles]):
                    articles.append({
                        'title': f'Art√≠culo {i+1}',
                        'url': link,
                        'content': '',
                        'summary': '',
                        'author': '',
                        'published_date': datetime.now().isoformat(),
                        'images_found': 0,
                        'images_downloaded': 0,
                        'images_data': []
                    })
                
                logging.info(f"‚úÖ M√©todo b√°sico: {len(articles)} art√≠culos encontrados")
            except Exception as e:
                logging.error(f"‚ùå Error en m√©todo b√°sico: {e}")
                articles = []
        
        # Guardar en base de datos
        if articles:
            if save_articles_to_db(articles, category, newspaper, region):
                logging.info(f"‚úÖ {len(articles)} art√≠culos guardados en base de datos")
            else:
                logging.error("‚ùå Error guardando art√≠culos en base de datos")
        
        logging.info(f"‚úÖ Scraping completado: {schedule['name']} - {len(articles)} art√≠culos")
        return True
        
    except Exception as e:
        logging.error(f"‚ùå Error ejecutando scraping {schedule['name']}: {e}")
        return False

def main():
    """Funci√≥n principal"""
    logging.info("üïê Iniciando scraping autom√°tico independiente")
    
    # Cargar configuraci√≥n
    config = load_config()
    if not config:
        return
    
    auto_config = config.get("auto_scraping", {})
    if not auto_config.get("enabled", False):
        logging.info("‚è∏Ô∏è Scraping autom√°tico deshabilitado")
        return
    
    # Ejecutar cada programaci√≥n
    schedules = auto_config.get("schedules", [])
    successful = 0
    failed = 0
    
    for schedule in schedules:
        if schedule.get("enabled", False):
            if execute_scraping_standalone(schedule):
                successful += 1
            else:
                failed += 1
            
            # Esperar entre ejecuciones para no sobrecargar
            time.sleep(30)
    
    logging.info(f"üìä Resumen: {successful} exitosos, {failed} fallidos")
    logging.info("üèÅ Scraping autom√°tico completado")

if __name__ == "__main__":
    main()
