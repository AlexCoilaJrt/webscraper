#!/usr/bin/env python3
"""
Scraper espec√≠fico para El Peruano usando Selenium
Maneja contenido din√°mico y paginaci√≥n
"""

import logging
import json
from datetime import datetime
from urllib.parse import urljoin, urlparse
import time
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service

logger = logging.getLogger(__name__)

class ElPeruanoSeleniumScraper:
    """Scraper espec√≠fico para El Peruano usando Selenium"""
    
    def __init__(self):
        self.driver = None
        self._setup_selenium()
    
    def _setup_selenium(self):
        """Configurar Selenium WebDriver"""
        try:
            chrome_options = Options()
            # chrome_options.add_argument('--headless')  # Comentado para debugging
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            service = Service('/usr/local/bin/chromedriver')
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.driver.set_page_load_timeout(30)
            logger.info("‚úÖ Selenium WebDriver configurado para El Peruano")
        except Exception as e:
            logger.error(f"‚ùå Error configurando Selenium: {e}")
            raise
    
    def scrape_economia_with_pagination(self, max_articles=50):
        """Scraper espec√≠fico para la secci√≥n de econom√≠a con paginaci√≥n"""
        url = "https://elperuano.pe/economia"
        
        try:
            logger.info(f"üîç Scrapeando El Peruano - Econom√≠a con Selenium: {url}")
            self.driver.get(url)
            time.sleep(5)  # Esperar carga inicial
            
            all_articles = []
            seen_urls = set()
            page_count = 0
            max_pages = 10  # L√≠mite de seguridad
            
            while len(all_articles) < max_articles and page_count < max_pages:
                page_count += 1
                logger.info(f"üìÑ Procesando p√°gina {page_count}")
                
                # Extraer art√≠culos de la p√°gina actual
                articles = self._extract_articles_from_current_page()
                new_articles = 0
                
                for article in articles:
                    if article.get('url') not in seen_urls and len(all_articles) < max_articles:
                        seen_urls.add(article.get('url'))
                        all_articles.append(article)
                        new_articles += 1
                
                logger.info(f"‚úÖ {new_articles} nuevos art√≠culos extra√≠dos de p√°gina {page_count}")
                
                # Buscar y hacer clic en "VER M√ÅS" o siguiente p√°gina
                if not self._navigate_to_next_page():
                    logger.info("‚ÑπÔ∏è No se encontr√≥ m√°s contenido, finalizando")
                    break
                
                time.sleep(3)  # Pausa entre p√°ginas
            
            logger.info(f"üéâ Total art√≠culos extra√≠dos: {len(all_articles)}")
            return all_articles
            
        except Exception as e:
            logger.error(f"‚ùå Error scrapeando El Peruano: {e}")
            return []
    
    def _extract_articles_from_current_page(self):
        """Extraer art√≠culos de la p√°gina actual"""
        articles = []
        
        try:
            # Esperar a que cargue el contenido
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # Buscar art√≠culos usando m√∫ltiples estrategias
            article_selectors = [
                # Selectores espec√≠ficos para El Peruano
                "article",
                ".noticia",
                ".articulo", 
                ".news-item",
                ".content-item",
                "div[class*='noticia']",
                "div[class*='articulo']",
                "div[class*='news']",
                "div[class*='content']",
                # Selectores m√°s gen√©ricos
                "h2 a",
                "h3 a", 
                "h4 a",
                ".title a",
                ".headline a",
                "a[href*='/noticia']",
                "a[href*='/articulo']",
                "a[href*='/economia']",
                # Selectores m√°s amplios
                "a[href*='elperuano.pe']",
                ".entry-title a",
                ".post-title a"
            ]
            
            found_links = set()
            
            for selector in article_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    logger.info(f"üîç Selector '{selector}': {len(elements)} elementos encontrados")
                    
                    for element in elements:
                        try:
                            # Si es un enlace directo
                            if element.tag_name == 'a':
                                link = element.get_attribute('href')
                                title = element.text.strip()
                            # Si es un contenedor con enlace
                            else:
                                link_elem = element.find_element(By.TAG_NAME, "a")
                                if link_elem:
                                    link = link_elem.get_attribute('href')
                                    title = link_elem.text.strip()
                                else:
                                    continue
                            
                            if not link or not title or len(title) < 10:
                                continue
                            
                            # Filtrar enlaces no relevantes
                            if any(skip in link.lower() for skip in ['javascript:', 'mailto:', '#', 'facebook', 'twitter', 'instagram', 'youtube']):
                                continue
                            
                            # Evitar duplicados
                            if link in found_links:
                                continue
                            found_links.add(link)
                            
                            # Extraer contenido del art√≠culo
                            article_data = self._extract_article_content_selenium(link, title)
                            if article_data:
                                articles.append(article_data)
                                logger.info(f"‚úÖ Art√≠culo extra√≠do: {title[:50]}...")
                                
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Error procesando elemento: {e}")
                            continue
                    
                    if len(articles) > 0:  # Si encontramos art√≠culos, no necesitamos m√°s selectores
                        break
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error con selector '{selector}': {e}")
                    continue
            
            logger.info(f"üìÑ Art√≠culos extra√≠dos de p√°gina actual: {len(articles)}")
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå Error extrayendo art√≠culos: {e}")
            return []
    
    def _navigate_to_next_page(self):
        """Navegar a la siguiente p√°gina"""
        try:
            # Buscar bot√≥n "VER M√ÅS" o enlaces de paginaci√≥n
            next_selectors = [
                "//button[contains(text(), 'VER M√ÅS')]",
                "//button[contains(text(), 'Cargar m√°s')]",
                "//button[contains(text(), 'M√°s')]",
                "//a[contains(text(), 'VER M√ÅS')]",
                "//a[contains(text(), 'Cargar m√°s')]",
                "//a[contains(text(), 'M√°s')]",
                "//a[contains(text(), 'Siguiente')]",
                "//a[contains(text(), '>')]",
                "//*[contains(@class, 'load-more')]",
                "//*[contains(@class, 'ver-mas')]",
                "//*[contains(@class, 'pagination')]//a[contains(text(), '>')]",
                "//*[contains(@class, 'pager')]//a[contains(text(), '>')]"
            ]
            
            for selector in next_selectors:
                try:
                    element = self.driver.find_element(By.XPATH, selector)
                    if element.is_displayed() and element.is_enabled():
                        # Hacer scroll hasta el elemento
                        self.driver.execute_script("arguments[0].scrollIntoView(true);", element)
                        time.sleep(1)
                        
                        # Hacer clic
                        self.driver.execute_script("arguments[0].click();", element)
                        logger.info(f"üîÑ Navegando a siguiente p√°gina: {selector}")
                        time.sleep(3)  # Esperar carga
                        return True
                        
                except NoSuchElementException:
                    continue
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error con selector '{selector}': {e}")
                    continue
            
            return False
            
        except Exception as e:
            logger.error(f"‚ùå Error navegando a siguiente p√°gina: {e}")
            return False
    
    def _extract_article_content_selenium(self, url, title):
        """Extraer contenido de un art√≠culo espec√≠fico usando Selenium"""
        try:
            # Abrir art√≠culo en nueva pesta√±a
            self.driver.execute_script(f"window.open('{url}', '_blank');")
            self.driver.switch_to.window(self.driver.window_handles[-1])
            time.sleep(2)
            
            # Extraer contenido
            content = ""
            content_selectors = [
                '.articulo-contenido',
                '.noticia-contenido', 
                '.article-content',
                '.content',
                '.texto',
                'article',
                '.main-content',
                'div[class*="contenido"]',
                'div[class*="texto"]',
                '.entry-content',
                '.post-content'
            ]
            
            for selector in content_selectors:
                try:
                    content_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if content_elem:
                        content = content_elem.text.strip()
                        break
                except NoSuchElementException:
                    continue
            
            # Si no se encuentra contenido espec√≠fico, usar todo el body
            if not content:
                try:
                    body = self.driver.find_element(By.TAG_NAME, 'body')
                    content = body.text.strip()
                except:
                    content = ""
            
            # Limpiar contenido
            content = re.sub(r'\s+', ' ', content)
            content = content[:2000]  # Limitar tama√±o
            
            # Extraer autor
            author = ""
            author_selectors = ['.autor', '.author', '.byline', '[class*="autor"]']
            for selector in author_selectors:
                try:
                    author_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if author_elem:
                        author = author_elem.text.strip()
                        break
                except NoSuchElementException:
                    continue
            
            # Extraer fecha
            date = ""
            date_selectors = ['.fecha', '.date', '.fecha-publicacion', '[class*="fecha"]']
            for selector in date_selectors:
                try:
                    date_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if date_elem:
                        date = date_elem.text.strip()
                        break
                except NoSuchElementException:
                    continue
            
            # Extraer im√°genes
            images = []
            try:
                img_elements = self.driver.find_elements(By.TAG_NAME, 'img')
                for img in img_elements:
                    img_src = img.get_attribute('src')
                    if img_src:
                        images.append(img_src)
            except:
                pass
            
            # Crear resumen
            summary = content[:200] + "..." if len(content) > 200 else content
            
            # Cerrar pesta√±a y volver a la principal
            self.driver.close()
            self.driver.switch_to.window(self.driver.window_handles[0])
            
            return {
                'title': title,
                'content': content,
                'summary': summary,
                'author': author,
                'date': date,
                'url': url,
                'newspaper': 'El Peruano',
                'category': 'Econom√≠a',
                'images_found': len(images),
                'images_downloaded': 0,
                'images_data': images,
                'scraped_at': datetime.now().isoformat(),
                'article_id': f"elperuano_{hash(url)}"
            }
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error extrayendo art√≠culo {url}: {e}")
            # Asegurar que volvemos a la pesta√±a principal
            try:
                if len(self.driver.window_handles) > 1:
                    self.driver.close()
                    self.driver.switch_to.window(self.driver.window_handles[0])
            except:
                pass
            return None
    
    def close(self):
        """Cerrar WebDriver"""
        if self.driver:
            self.driver.quit()

def scrape_elperuano_economia_selenium(max_articles=50):
    """Funci√≥n principal para scrapear El Peruano - Econom√≠a con Selenium"""
    scraper = ElPeruanoSeleniumScraper()
    try:
        articles = scraper.scrape_economia_with_pagination(max_articles)
        return articles
    finally:
        scraper.close()

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    articles = scrape_elperuano_economia_selenium(10)
    print(f"Art√≠culos extra√≠dos: {len(articles)}")
    for article in articles:
        print(f"- {article['title']}")

