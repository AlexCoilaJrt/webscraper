#!/usr/bin/env python3
"""
Script para configurar scraping autom√°tico
"""

import os
import sys
import json
from datetime import datetime, timedelta

def create_auto_scraping_config():
    """Crear configuraci√≥n para scraping autom√°tico"""
    
    config = {
        "auto_scraping": {
            "enabled": True,
            "schedules": [
                {
                    "name": "Noticias Matutinas",
                    "url": "https://elcomercio.pe/",
                    "method": "hybrid",
                    "max_articles": 20,
                    "max_images": 10,
                    "category": "General",
                    "newspaper": "El Comercio",
                    "region": "Nacional",
                    "cron_schedule": "0 8 * * *",  # Todos los d√≠as a las 8:00 AM
                    "enabled": True
                },
                {
                    "name": "Noticias Vespertinas",
                    "url": "https://elpopular.pe/",
                    "method": "optimized",
                    "max_articles": 15,
                    "max_images": 8,
                    "category": "General",
                    "newspaper": "El Popular",
                    "region": "Nacional",
                    "cron_schedule": "0 18 * * *",  # Todos los d√≠as a las 6:00 PM
                    "enabled": True
                },
                {
                    "name": "Noticias Diario Sin Fronteras",
                    "url": "https://diariosinfronteras.com.pe/",
                    "method": "hybrid",
                    "max_articles": 25,
                    "max_images": 15,
                    "category": "Regional",
                    "newspaper": "Diario Sin Fronteras",
                    "region": "Nacional",
                    "cron_schedule": "0 12 * * *",  # Todos los d√≠as a las 12:00 PM
                    "enabled": True
                }
            ]
        }
    }
    
    # Guardar configuraci√≥n
    with open('auto_scraping_config.json', 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print("‚úÖ Configuraci√≥n de scraping autom√°tico creada")
    return config

def create_cron_script():
    """Crear script para ejecutar desde cron"""
    
    script_content = '''#!/bin/bash
# Script de scraping autom√°tico
# Ejecutar desde cron job

# Cambiar al directorio del proyecto
cd /Users/usuario/Documents/scraping\ 2

# Activar entorno virtual si existe
if [ -d "venv" ]; then
    source venv/bin/activate
fi

# Ejecutar scraping autom√°tico
python auto_scraper.py

# Log de ejecuci√≥n
echo "$(date): Scraping autom√°tico ejecutado" >> auto_scraping.log
'''
    
    with open('run_auto_scraping.sh', 'w') as f:
        f.write(script_content)
    
    # Hacer ejecutable
    os.chmod('run_auto_scraping.sh', 0o755)
    
    print("‚úÖ Script de cron creado: run_auto_scraping.sh")

def create_auto_scraper():
    """Crear el script principal de scraping autom√°tico"""
    
    script_content = '''#!/usr/bin/env python3
"""
Scraper autom√°tico - Ejecuta scraping programado
"""

import json
import requests
import logging
from datetime import datetime
import time

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('auto_scraping.log'),
        logging.StreamHandler()
    ]
)

def load_config():
    """Cargar configuraci√≥n de scraping autom√°tico"""
    try:
        with open('auto_scraping_config.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logging.error("‚ùå Archivo de configuraci√≥n no encontrado")
        return None

def execute_scraping(schedule):
    """Ejecutar scraping para una programaci√≥n espec√≠fica"""
    try:
        logging.info(f"üöÄ Iniciando scraping: {schedule['name']}")
        
        # Preparar datos para la API
        data = {
            "url": schedule["url"],
            "max_articles": schedule["max_articles"],
            "max_images": schedule["max_images"],
            "method": schedule["method"],
            "download_images": True,
            "category": schedule["category"],
            "newspaper": schedule["newspaper"],
            "region": schedule["region"]
        }
        
        # Llamar a la API
        response = requests.post(
            "http://localhost:5001/api/start-scraping",
            json=data,
            timeout=300  # 5 minutos timeout
        )
        
        if response.status_code == 200:
            result = response.json()
            logging.info(f"‚úÖ Scraping completado: {schedule['name']}")
            logging.info(f"   - Art√≠culos: {result.get('status', {}).get('articles_found', 0)}")
            logging.info(f"   - Im√°genes: {result.get('status', {}).get('images_found', 0)}")
            return True
        else:
            logging.error(f"‚ùå Error en scraping {schedule['name']}: {response.status_code}")
            return False
            
    except Exception as e:
        logging.error(f"‚ùå Error ejecutando scraping {schedule['name']}: {e}")
        return False

def main():
    """Funci√≥n principal"""
    logging.info("üïê Iniciando scraping autom√°tico")
    
    # Cargar configuraci√≥n
    config = load_config()
    if not config:
        return
    
    auto_config = config.get("auto_scraping", {})
    if not auto_config.get("enabled", False):
        logging.info("‚è∏Ô∏è Scraping autom√°tico deshabilitado")
        return
    
    # Ejecutar cada programaci√≥n
    schedules = auto_config.get("schedules", [])
    successful = 0
    failed = 0
    
    for schedule in schedules:
        if schedule.get("enabled", False):
            if execute_scraping(schedule):
                successful += 1
            else:
                failed += 1
            
            # Esperar entre ejecuciones para no sobrecargar
            time.sleep(30)
    
    logging.info(f"üìä Resumen: {successful} exitosos, {failed} fallidos")
    logging.info("üèÅ Scraping autom√°tico completado")

if __name__ == "__main__":
    main()
'''
    
    with open('auto_scraper.py', 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    print("‚úÖ Script de scraping autom√°tico creado: auto_scraper.py")

def show_cron_instructions():
    """Mostrar instrucciones para configurar cron"""
    
    instructions = """
üîß INSTRUCCIONES PARA CONFIGURAR CRON JOBS:

1. üìù Abrir crontab:
   crontab -e

2. üìÖ Agregar estas l√≠neas (ajusta los horarios seg√∫n necesites):
   
   # Scraping autom√°tico - Noticias matutinas (8:00 AM)
   0 8 * * * /Users/usuario/Documents/scraping\\ 2/run_auto_scraping.sh
   
   # Scraping autom√°tico - Noticias vespertinas (6:00 PM)
   0 18 * * * /Users/usuario/Documents/scraping\\ 2/run_auto_scraping.sh
   
   # Scraping autom√°tico - Diario Sin Fronteras (12:00 PM)
   0 12 * * * /Users/usuario/Documents/scraping\\ 2/run_auto_scraping.sh

3. üíæ Guardar y salir (Ctrl+X, luego Y, luego Enter)

4. ‚úÖ Verificar que se guard√≥:
   crontab -l

üìã FORMATO DE CRON:
   minuto hora d√≠a mes d√≠a_semana comando
   
   Ejemplos:
   - 0 8 * * *     = Todos los d√≠as a las 8:00 AM
   - 0 */6 * * *   = Cada 6 horas
   - 30 9 * * 1-5  = Lunes a Viernes a las 9:30 AM
   - 0 0 1 * *     = Primer d√≠a de cada mes a medianoche

üîç VERIFICAR LOGS:
   tail -f auto_scraping.log
"""
    
    print(instructions)

def main():
    """Funci√≥n principal"""
    print("üöÄ Configurando scraping autom√°tico...")
    
    # Crear archivos necesarios
    create_auto_scraping_config()
    create_cron_script()
    create_auto_scraper()
    
    # Mostrar instrucciones
    show_cron_instructions()
    
    print("\n‚úÖ ¬°Configuraci√≥n completada!")
    print("üìÅ Archivos creados:")
    print("   - auto_scraping_config.json (configuraci√≥n)")
    print("   - auto_scraper.py (script principal)")
    print("   - run_auto_scraping.sh (script de cron)")
    print("   - auto_scraping.log (logs)")

if __name__ == "__main__":
    main()

