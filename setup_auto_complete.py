#!/usr/bin/env python3
"""
Script completo para configurar scraping automÃ¡tico
"""

import sqlite3
import json
import os
from datetime import datetime

def create_database():
    """Crear base de datos SQLite con las tablas necesarias"""
    try:
        conn = sqlite3.connect('scraping_data.db')
        cursor = conn.cursor()
        
        # Crear tabla de artÃ­culos
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                content TEXT,
                url TEXT UNIQUE,
                summary TEXT,
                author TEXT,
                published_date TEXT,
                scraped_at TEXT,
                category TEXT,
                newspaper TEXT,
                region TEXT,
                images_found INTEGER DEFAULT 0,
                images_downloaded INTEGER DEFAULT 0,
                images_data TEXT
            )
        ''')
        
        # Crear tabla de imÃ¡genes
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS images (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                article_id INTEGER,
                image_url TEXT,
                filename TEXT,
                downloaded_at TEXT,
                FOREIGN KEY (article_id) REFERENCES articles (id)
            )
        ''')
        
        # Crear tabla de estadÃ­sticas
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS scraping_stats (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id TEXT,
                url_scraped TEXT,
                articles_found INTEGER,
                images_found INTEGER,
                images_downloaded INTEGER,
                duration_seconds INTEGER,
                method_used TEXT,
                created_at TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
        print("âœ… Base de datos creada exitosamente")
        return True
        
    except Exception as e:
        print(f"âŒ Error creando base de datos: {e}")
        return False

def create_cron_setup():
    """Crear script para configurar cron jobs fÃ¡cilmente"""
    
    script_content = '''#!/bin/bash
# Script para configurar cron jobs de scraping automÃ¡tico

echo "ğŸ”§ Configurando scraping automÃ¡tico..."

# Crear backup del crontab actual
crontab -l > crontab_backup_$(date +%Y%m%d_%H%M%S).txt 2>/dev/null || echo "No hay crontab existente"

# Agregar nuevos cron jobs
(crontab -l 2>/dev/null; echo "") | crontab -
(crontab -l 2>/dev/null; echo "# Scraping automÃ¡tico - Noticias matutinas (8:00 AM)") | crontab -
(crontab -l 2>/dev/null; echo "0 8 * * * /Users/usuario/Documents/scraping\\ 2/run_auto_scraping.sh") | crontab -
(crontab -l 2>/dev/null; echo "") | crontab -
(crontab -l 2>/dev/null; echo "# Scraping automÃ¡tico - Noticias vespertinas (6:00 PM)") | crontab -
(crontab -l 2>/dev/null; echo "0 18 * * * /Users/usuario/Documents/scraping\\ 2/run_auto_scraping.sh") | crontab -
(crontab -l 2>/dev/null; echo "") | crontab -
(crontab -l 2>/dev/null; echo "# Scraping automÃ¡tico - Diario Sin Fronteras (12:00 PM)") | crontab -
(crontab -l 2>/dev/null; echo "0 12 * * * /Users/usuario/Documents/scraping\\ 2/run_auto_scraping.sh") | crontab -

echo "âœ… Cron jobs configurados exitosamente"
echo "ğŸ“‹ Para ver los cron jobs configurados: crontab -l"
echo "ğŸ“‹ Para ver los logs: tail -f auto_scraping.log"
echo "ğŸ“‹ Para eliminar todos los cron jobs: crontab -r"
'''
    
    with open('setup_cron.sh', 'w') as f:
        f.write(script_content)
    
    os.chmod('setup_cron.sh', 0o755)
    print("âœ… Script de configuraciÃ³n de cron creado: setup_cron.sh")

def create_management_script():
    """Crear script de gestiÃ³n del scraping automÃ¡tico"""
    
    script_content = '''#!/usr/bin/env python3
"""
Script de gestiÃ³n del scraping automÃ¡tico
"""

import json
import sys
import os

def show_status():
    """Mostrar estado del scraping automÃ¡tico"""
    try:
        with open('auto_scraping_config.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        auto_config = config.get("auto_scraping", {})
        print(f"ğŸ”„ Scraping automÃ¡tico: {'âœ… Habilitado' if auto_config.get('enabled') else 'âŒ Deshabilitado'}")
        
        schedules = auto_config.get("schedules", [])
        print(f"ğŸ“… Programaciones configuradas: {len(schedules)}")
        
        for i, schedule in enumerate(schedules, 1):
            status = "âœ…" if schedule.get("enabled") else "âŒ"
            print(f"   {i}. {status} {schedule['name']} - {schedule['cron_schedule']}")
        
        # Verificar archivos
        files = [
            'auto_scraping_config.json',
            'auto_scraper_standalone.py',
            'run_auto_scraping.sh',
            'scraping_data.db'
        ]
        
        print("\\nğŸ“ Archivos del sistema:")
        for file in files:
            exists = "âœ…" if os.path.exists(file) else "âŒ"
            print(f"   {exists} {file}")
        
    except Exception as e:
        print(f"âŒ Error mostrando estado: {e}")

def enable_disable(enabled):
    """Habilitar o deshabilitar scraping automÃ¡tico"""
    try:
        with open('auto_scraping_config.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        config["auto_scraping"]["enabled"] = enabled
        
        with open('auto_scraping_config.json', 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        status = "habilitado" if enabled else "deshabilitado"
        print(f"âœ… Scraping automÃ¡tico {status}")
        
    except Exception as e:
        print(f"âŒ Error: {e}")

def run_now():
    """Ejecutar scraping ahora"""
    print("ğŸš€ Ejecutando scraping automÃ¡tico...")
    os.system("python auto_scraper_standalone.py")

def show_logs():
    """Mostrar logs recientes"""
    try:
        with open('auto_scraping.log', 'r') as f:
            lines = f.readlines()
            print("ğŸ“‹ Ãšltimas 20 lÃ­neas del log:")
            for line in lines[-20:]:
                print(line.strip())
    except FileNotFoundError:
        print("âŒ Archivo de log no encontrado")

def main():
    """FunciÃ³n principal"""
    if len(sys.argv) < 2:
        print("""
ğŸ”§ GestiÃ³n del Scraping AutomÃ¡tico

Uso: python manage_auto_scraping.py [comando]

Comandos disponibles:
  status     - Mostrar estado del sistema
  enable     - Habilitar scraping automÃ¡tico
  disable    - Deshabilitar scraping automÃ¡tico
  run        - Ejecutar scraping ahora
  logs       - Mostrar logs recientes
  help       - Mostrar esta ayuda
""")
        return
    
    command = sys.argv[1].lower()
    
    if command == "status":
        show_status()
    elif command == "enable":
        enable_disable(True)
    elif command == "disable":
        enable_disable(False)
    elif command == "run":
        run_now()
    elif command == "logs":
        show_logs()
    elif command == "help":
        main()
    else:
        print(f"âŒ Comando desconocido: {command}")
        print("Usa 'python manage_auto_scraping.py help' para ver comandos disponibles")

if __name__ == "__main__":
    main()
'''
    
    with open('manage_auto_scraping.py', 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    os.chmod('manage_auto_scraping.py', 0o755)
    print("âœ… Script de gestiÃ³n creado: manage_auto_scraping.py")

def main():
    """FunciÃ³n principal"""
    print("ğŸš€ Configurando scraping automÃ¡tico completo...")
    
    # Crear base de datos
    if not create_database():
        return
    
    # Crear scripts de gestiÃ³n
    create_cron_setup()
    create_management_script()
    
    print("""
âœ… Â¡ConfiguraciÃ³n completada!

ğŸ“‹ PRÃ“XIMOS PASOS:

1. ğŸ”§ Configurar cron jobs:
   ./setup_cron.sh

2. ğŸ® Gestionar el sistema:
   python manage_auto_scraping.py status
   python manage_auto_scraping.py run
   python manage_auto_scraping.py logs

3. ğŸ“… Los scraping se ejecutarÃ¡n automÃ¡ticamente:
   - 8:00 AM - Noticias matutinas
   - 12:00 PM - Diario Sin Fronteras  
   - 6:00 PM - Noticias vespertinas

4. ğŸ“Š Verificar resultados:
   - Logs: tail -f auto_scraping.log
   - Base de datos: scraping_data.db
   - Estado: python manage_auto_scraping.py status

ğŸ‰ Â¡Tu sistema de scraping automÃ¡tico estÃ¡ listo!
""")

if __name__ == "__main__":
    main()

